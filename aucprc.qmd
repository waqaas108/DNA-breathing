---
title: AUC and PRC comparisons across TFs
filters:
   - lightbox
lightbox: auto
tbl-cap-location: bottom
---

<style>
  code {
    white-space : pre-wrap !important;
    word-break: break-word;
  }
</style>

## Model Parameter selection

We had previously used a kernel size of 5 for the CNNs, but we had planned to use the weights of our CNNs as PWMs for motif extraction, and a kernel size of 5 would not be ideal for this purpose. We decided to contrast the performance of our model with models of other kernel sizes. We trained models with kernel sizes of 7 and 9, and compared their performance with the model with kernel size 5.

::: {#fig-kernel_comparison layout-ncol=3}
![](figs/aucprc/microauc.png){group="kernel_comparison"}

![](figs/aucprc/macroauc.png){group="kernel_comparison"}

![](figs/aucprc/prc.png){group="kernel_comparison"}

Curves for the micro-averaged AUC, macro-averaged ARC, and PRC for different kernel sizes. The kernel size of 5 is the best performing model, with 7 being comparable. A kernel size of 9 is significantly worse.
:::


## Nullification method selection

We also compared the performance of our model with different nullification methods. We had previously used a comparison between the hybrid LSTM model and the simple sequence only CNN model to determine that YY1's prediction scores were highly affected by the presence of biophysical features. We decided to review this conclusion by testing different nullification methods on the hybrid LSTM architecture and comparing their performance for each TF.

::: {#fig-z_i_nullification_comparison layout-ncol=2}
![](figs/aucprc/znull/auc.png){group="z_i_nullification_comparison"}

![](figs/aucprc/znull/prc.png){group="z_i_nullification_comparison"}

Zero nullification at inference time.
:::

::: {#fig-g_i_nullification_comparison layout-ncol=2}
![](figs/aucprc/gnull/auc.png){group="g_i_nullification_comparison"}

![](figs/aucprc/gnull/prc.png){group="g_i_nullification_comparison"}

Gaussian nullification at inference time.
:::

::: {#fig-z_t_nullification_comparison layout-ncol=2}
![](figs/aucprc/dualmodel_z/auc.png){group="z_t_nullification_comparison"}

![](figs/aucprc/dualmodel_z/prc.png){group="z_t_nullification_comparison"}

Zero nullification at training time.
:::

::: {#fig-g_t_nullification_comparison layout-ncol=2}
![](figs/aucprc/dualmodel_g/auc.png){group="g_t_nullification_comparison"}

![](figs/aucprc/dualmodel_g/prc.png){group="g_t_nullification_comparison"}

Gaussian nullification at training time.
:::

Nullification at inference time produces drastic differences in the scores which may not be representative of an actual improvement in predictive power. Gaussian nullification at training time was considered as the best approach.


## YY1, STAT5A, and FOSL1

### Score comparison of each TF

Since the PRCs showed greater differences than AUCs, we focused on comparing the PRC of each TF at training time (both gaussian and zero nullification methods). We found that YY1 was overshadowed by other TFs such as STAT5A and FOSL1, which had higher PRCs when biophysical features were included.

::: {#fig-yy1_stat5a_fosl1_g layout="[50, -5, 50]"}
![](figs/aucprc/scatter/prcg.png){group="yy1_stat5a_fosl1_g"}

![](figs/aucprc/scatter/prcg_bar.png){group="yy1_stat5a_fosl1_g"}

Gaussian nullification at training time.
:::

::: {#fig-yy1_stat5a_fosl1_z layout="[50, -5, 50]"}
![](figs/aucprc/scatter/prcnull.png){group="yy1_stat5a_fosl1_z"}

![](figs/aucprc/scatter/prcnull_bar.png){group="yy1_stat5a_fosl1_z"}

Zero nullification at training time.
:::

We find that YY1 (red dot) is overshadowed by STAT5A and FOSL1 (green triangles) in both nullification methods.



::: {#fig-yy1_stat5a_fosl1_auc_prc layout="[60, -5, 60, -5, 60]"}
![YY1](figs/aucprc/tfs/yy1.png){group="yy1_stat5a_fosl1_auc_prc"}

![STAT5A](figs/aucprc/tfs/stat5a.png){group="yy1_stat5a_fosl1_auc_prc"}

![FOSL1](figs/aucprc/tfs/fosl1.png){group="yy1_stat5a_fosl1_auc_prc"}

Line graphs for the AUC and PRC of each TF. YY1 has lower differences between the models compared to STAT5A and FOSL1. Nullification method is gaussian nullification at training time.
:::


### Gaussian noise model simulation

To further confirm the results for STAT5A and FOSL1, we created 5 different datasets of gaussian noise and trained a different model on each dataset. We then compared the PRCs of each model against a model with actual biophysical features. The results showed that the models trained on gaussian noise had lower and more variable PRCs.

::: {#fig-gaussian_noise layout-ncol=2}
![](figs/aucprc/tfs/auc_sim.png){group="gaussian_noise"}

![](figs/aucprc/tfs/prc_sim.png){group="gaussian_noise"}

Bar graphs for the AUC and PRC of each TF for models trained on gaussian noise. The models trained on gaussian noise had lower and more variable PRCs.
:::

## Motif extraction

As an example, we extracted motifs using our CNN model with a kernel size of 9. The motifs had the added benefit of containing 3 tracks corresponding to the 3 biophysical features.

::: {#fig-motif_extraction}
![](figs/aucprc/motif.png){group="motif_extraction"}

Motif extraction using the CNN model with kernel size 9. The motifs contain 3 tracks corresponding to the 3 biophysical features.
:::

We did not continue this line of inquiry due to the poor performance of the model with kernel size 9.
